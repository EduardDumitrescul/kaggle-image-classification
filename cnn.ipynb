{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48243f74-2bd0-4f45-971d-406327ee9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for loding PNG\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3e2d6a-8312-4bf7-afa2-4378ca1a620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(name):\n",
    "    df = pd.read_csv(name + '.csv')\n",
    "    df_sorted = df.sort_values(by='image_id')\n",
    "    #labels = df_sorted.drop(columns=['image_id'])\n",
    "\n",
    "    labelDict = dict()\n",
    "    labels = df_sorted.values\n",
    "    for entry in labels:\n",
    "        try:\n",
    "            labelDict[entry[0]] = entry[1]\n",
    "        except:\n",
    "            labelDict[entry[0]] = 0\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(name):\n",
    "        if filename.endswith('.png'):\n",
    "            file_path = os.path.join(name, filename)\n",
    "            image = Image.open(file_path)\n",
    "            image_array = np.array(image)\n",
    "            if filename[:-4] in labelDict.keys():\n",
    "                filenames.append(filename[:-4])\n",
    "                images.append(image_array)\n",
    "                labels.append(labelDict[filename[:-4]])\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        if images[i].shape != (80, 80, 3):\n",
    "            # images[i] = np.mean(images[i], axis=2, keepdims=True)\n",
    "            images[i] = np.stack((images[i], images[i], images[i]), axis=-1)\n",
    "        # else:\n",
    "        #     images[i] = np.reshape(images[i], (80, 80, 1))\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    means = np.mean(images, axis=(0, 1, 2), keepdims=True)\n",
    "    stds = np.std(images, axis=(0, 1, 2), keepdims=True)\n",
    "    images = (images - means) / stds\n",
    "\n",
    "    mins = np.min(images, axis=(1, 2), keepdims=True)\n",
    "    maxs = np.max(images, axis=(1, 2), keepdims=True)\n",
    "    images = (images - mins) / (maxs - mins) * 255.0\n",
    "\n",
    "    return filenames, images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5ad9d7-2fd8-45fa-8e7e-9e3610778178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 80, 80, 3)\n",
      "(10500,)\n",
      "(4500, 80, 80, 3)\n",
      "(4500,)\n",
      "(3000, 80, 80, 3)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "#LOAD\n",
    "test_filenames, test_images, test_labels = load_data('test')\n",
    "train_filenames, train_images, train_labels = load_data('train')\n",
    "validation_filenames, validation_images, validation_labels = load_data('validation')\n",
    "\n",
    "train_images_subset_train, validation_images_subset_train, train_labels_subset_train, validation_labels_subset_train = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "validation_images = np.array(validation_images)\n",
    "\n",
    "# Assuming train_images, train_labels, validation_images, validation_labels are already loaded\n",
    "# Ensure the labels are one-hot encoded\n",
    "train_labels_one_hot = tf.keras.utils.to_categorical(train_labels, num_classes=3)\n",
    "validation_labels_one_hot = tf.keras.utils.to_categorical(validation_labels, num_classes=3)\n",
    "\n",
    "train_labels_subset_train_one_hot = tf.keras.utils.to_categorical(train_labels_subset_train, num_classes=3)\n",
    "validation_labels_subset_train_one_hot = tf.keras.utils.to_categorical(validation_labels_subset_train, num_classes=3)\n",
    "\n",
    "print(np.shape(train_images))\n",
    "print(np.shape(train_labels))\n",
    "print(np.shape(test_images))\n",
    "print(np.shape(test_labels))\n",
    "print(np.shape(validation_images))\n",
    "print(np.shape(validation_labels))\n",
    "\n",
    "Table = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa4aa9-6bea-4e4b-bc26-5b7d458d60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(f'Training class distribution: {dict(zip(unique, counts))}')\n",
    "unique, counts = np.unique(validation_labels, return_counts=True)\n",
    "print(f'Validation class distribution: {dict(zip(unique, counts))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a5b1f-bebe-4807-a8e1-32d3d363f1d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):\n",
    "    # print(train_images[i])\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.title(f'Training Label: {train_labels[i]}')\n",
    "    plt.show()\n",
    "\n",
    "for i in range(5):\n",
    "    # print(validation_images[i])\n",
    "    plt.imshow(np.uint8(validation_images[i]))\n",
    "    plt.title(f'Validation Label: {validation_labels[i]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2818b77-063b-494e-922f-cb6ac588dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(image, stddev=0.1):\n",
    "    noise = np.random.normal(0, stddev, image.shape)\n",
    "    noisy_image = image + noise\n",
    "    noisy_image = np.clip(noisy_image, 0., 1.)\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d728f1-6590-43db-b960-87f0e914a176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "model_train_images = train_images\n",
    "model_train_labels = train_labels_one_hot\n",
    "model_validation_images = validation_images\n",
    "model_validation_labels = validation_labels_one_hot\n",
    "\n",
    "\n",
    "# Data augmentation to prevent overfitting\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Fit the data generator to the training data\n",
    "datagen.fit(model_train_images)\n",
    "\n",
    "# Define the model\n",
    "my_model = tf.keras.models.Sequential([\n",
    " Conv2D(16, (3,3), activation='relu' ,\n",
    " input_shape=(80, 80, 3)),\n",
    " MaxPooling2D(2, 2),\n",
    " Conv2D(32, (3,3), activation='relu'),\n",
    " MaxPooling2D(2,2),\n",
    " Conv2D(64, (3,3), activation='relu'),\n",
    " MaxPooling2D(2,2),\n",
    " Conv2D(64, (3,3), activation='relu'),\n",
    " MaxPooling2D(2,2),\n",
    " Conv2D(64, (3,3), activation='relu'),\n",
    " Flatten(),\n",
    " Dense(512, activation='relu'),\n",
    " Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "my_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with data augmentation\n",
    "history = my_model.fit(\n",
    "    # datagen.flow(model_train_images, model_train_labels, batch_size=32),\n",
    "    model_train_images, model_train_labels,\n",
    "    # batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=(model_validation_images, model_validation_labels),\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = my_model.evaluate(model_validation_images, model_validation_labels)\n",
    "print(f'Validation accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af21adc-8ab6-4642-844c-a51c76227d15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model_train_images = train_images\n",
    "model_train_labels = train_labels_one_hot\n",
    "model_validation_images = validation_images\n",
    "model_validation_labels = validation_labels_one_hot\n",
    "\n",
    "base_model = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(80, 80, 3))\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "#         layer.filters = int(layer.filters * 0.5)  # Reduce the number of filters by 25%\n",
    "\n",
    "# Add custom layers on top of the modified base model\n",
    "x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(3, activation='softmax')(x)  # Assuming num_classes is the number of output classes\n",
    "\n",
    "# Create the model\n",
    "eff_net_b4_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "eff_net_b4_model.compile(optimizer=Adam(learning_rate=2e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "eff_net_b4_model.fit(model_train_images, model_train_labels, epochs=5, validation_data=(model_validation_images, model_validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "987187e5-99e5-48e0-bfdd-d460794c303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 21:23:00.479430: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2024-05-28 21:23:00.479463: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:134] retrieving CUDA diagnostic information for host: eduard-IdeaPad-5-Pro-16ACH6\n",
      "2024-05-28 21:23:00.479471: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:141] hostname: eduard-IdeaPad-5-Pro-16ACH6\n",
      "2024-05-28 21:23:00.479653: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:165] libcuda reported version is: 555.42.2\n",
      "2024-05-28 21:23:00.479676: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:169] kernel reported version is: 535.171.4\n",
      "2024-05-28 21:23:00.479682: E external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:251] kernel version 535.171.4 does not match DSO version 555.42.2 -- cannot find working devices in this configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 21:23:01.620924: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 806400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 961ms/step - accuracy: 0.4331 - loss: 1.5224"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 21:28:35.846874: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 230400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 997ms/step - accuracy: 0.4332 - loss: 1.5216 - val_accuracy: 0.5493 - val_loss: 0.9720\n",
      "Epoch 2/5\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 1s/step - accuracy: 0.5986 - loss: 0.8645 - val_accuracy: 0.6097 - val_loss: 0.8746\n",
      "Epoch 3/5\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 1s/step - accuracy: 0.7075 - loss: 0.6799 - val_accuracy: 0.6193 - val_loss: 0.8781\n",
      "Epoch 4/5\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 1s/step - accuracy: 0.7861 - loss: 0.5131 - val_accuracy: 0.5767 - val_loss: 1.0221\n",
      "Epoch 5/5\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 1s/step - accuracy: 0.8304 - loss: 0.4059 - val_accuracy: 0.6110 - val_loss: 1.0279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7df3ba0d65a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model_train_images = train_images\n",
    "model_train_labels = train_labels_one_hot\n",
    "model_validation_images = validation_images\n",
    "model_validation_labels = validation_labels_one_hot\n",
    "\n",
    "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(80, 80, 3))\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "#         layer.filters = int(layer.filters * 0.5)  # Reduce the number of filters by 25%\n",
    "# for layer in base_model.layers:\n",
    "#     if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "#         layer.filters = int(layer.filters * 0.5)\n",
    "\n",
    "# num_blocks_to_remove = 5  # Example: Remove the last 5 blocks\n",
    "# for _ in range(num_blocks_to_remove):\n",
    "#     base_model.layers.pop()\n",
    "\n",
    "# Add custom layers on top of the modified base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(3, activation='softmax')(x)  # Assuming num_classes is the number of output classes\n",
    "\n",
    "# Create the model\n",
    "res_net_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "res_net_model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "res_net_model.fit(model_train_images, model_train_labels, epochs=5, validation_data=(model_validation_images, model_validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c003a3-d62a-483e-972c-0ef2ddd37902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-28 22:01:21.547522: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 230400000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/94\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 124ms/step"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = base_model\n",
    "\n",
    "y_prediction = model.predict(validation_images)\n",
    "y_prediction = np.argmax (y_prediction, axis = 1)\n",
    "# y_test=np.argmax(validation_labels, axis=1)\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(validation_labels, y_prediction , normalize='pred')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce99fc4-ca6e-433a-8b69-d23992b1c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# history = model.fit(\n",
    "#     datagen.flow(train_images, train_labels_one_hot),\n",
    "#     epochs=50,\n",
    "#     validation_data=(validation_images, validation_labels_one_hot),\n",
    "#     callbacks=[early_stopping]\n",
    "# )\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545cbd5b-84e2-4ec7-b09a-3d24eb9cc670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "validation_labels_one_hot = tf.keras.utils.to_categorical(validation_labels, num_classes=3)\n",
    "\n",
    "model.evaluate(validation_images, validation_labels_one_hot)\n",
    "model.evaluate(train_images, train_labels_one_hot)\n",
    "predict = model.predict(validation_images)\n",
    "\n",
    "print(validation_labels_one_hot)\n",
    "\n",
    "for i in range(len(validation_labels_one_hot)):\n",
    "    print(f'{validation_labels_one_hot[i]} - {np.round(predict[i])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e667a-140f-4947-8167-0f5b42a992f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def printPrediction(filenames, prediction, name):\n",
    "    d = dict()\n",
    "    for i in range(len(filenames)):\n",
    "        d[filenames[i]] = prediction[i]\n",
    "\n",
    "\n",
    "    df = pd.read_csv('sample_submission.csv')\n",
    "    for i in range(len(filenames)):\n",
    "        df.at[i, 'label'] = d[df.at[i, 'image_id']]\n",
    "    df.to_csv(name, index=False)\n",
    "\n",
    "prediction = my_model.predict(test_images)\n",
    "prediction = np.argmax (prediction, axis = 1)\n",
    "\n",
    "print(*zip(prediction, test_filenames), sep='\\n')\n",
    "\n",
    "printPrediction(test_filenames, prediction, 'cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635d53f-8e3f-4d83-a837-42a844b46219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
